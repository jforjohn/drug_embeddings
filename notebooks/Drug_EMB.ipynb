{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "nyR7xY2uE7KK",
    "outputId": "8cded9b8-2a4a-4263-e6e3-6da0d3fbd266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading https://files.pythonhosted.org/packages/55/dd/3bf1c646c310daabae47fceb84ea9ab66df7f518a31a89955290d82b8100/seqeval-0.0.10-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.16.3)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.7)\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-0.0.10\n",
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-nvfzmhhw\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-nvfzmhhw\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.16.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.2.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wmj7w1gi/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
      "Successfully built keras-contrib\n",
      "Installing collected packages: keras-contrib\n",
      "Successfully installed keras-contrib-2.0.8\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/jforjohn/drug_embeddings\n",
    "!pip install seqeval\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "relgNEQmqaPU",
    "outputId": "7b1f4909-8235-4b3f-ceb7-6651faa76b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "!python /content/gdrive/My\\ Drive/drug_embeddings/src/MainLauncher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qN6p2JNAaAp5",
    "outputId": "f6a05c28-e710-48e0-86b8-ffcb66613c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "from os import chdir; chdir('/content/gdrive/My Drive/drug_embeddings/src')\n",
    "import sys; sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "WXtgjd_P33WY",
    "outputId": "b041c7dc-1626-4254-d1c5-705cd519c144"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "2.2.4\n",
      "Directory  model1  already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from input_output.parser import Parser\n",
    "from input_output.writer import Writer\n",
    "from input_output.load_config import load_config_file\n",
    "from preprocessing.tokenizer import tokenize\n",
    "from preprocessing.transformations import removeEmptyRows\n",
    "from preprocessing.transformations import CRF_get_tag\n",
    "from structs import DrugEntity\n",
    "from models.dl import architecture\n",
    "from models.dl import Metrics\n",
    "from models.dl import embedding_weights\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras_contrib import metrics, losses\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from os import mkdir\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "def preprocess_steps(base_folder):\n",
    "  df = Parser('../'+ base_folder).call()\n",
    "  #df['sentence'] = df['sentence'].apply(sentClean)#.apply(lambda x: tokens2sent(x, removals=False))\n",
    "  df['tokens'] = df['sentence'].apply(tokenize)\n",
    "  df['crf_tags'] = df[['tokens', 'parsed_drugs']].apply(CRF_get_tag, axis=1)\n",
    "  return df\n",
    "    \n",
    "config = load_config_file('config', './')\n",
    "config_data = config['data']\n",
    "config_preprocess = config['preprocessing']\n",
    "config_arch = config['arch']\n",
    "config_training = config['training']\n",
    "\n",
    "output_dir = config_data['output_dir']\n",
    "pretrained_emb_dir = config_data['pretrained_emb_dir']\n",
    "try:\n",
    "    # Create target Directory\n",
    "    mkdir(output_dir)\n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , output_dir ,  \" already exists\")\n",
    "    \n",
    "train_base_folder = config_data.get('train_dir')\n",
    "test_base_folder = config_data.get('test_dir')\n",
    "\n",
    "df_train = preprocess_steps(train_base_folder)\n",
    "df_test = preprocess_steps(test_base_folder)\n",
    "\n",
    "emb_dim = config_preprocess['EMB_DIM']\n",
    "max_len = df_train['tokens'].apply(len).max()\n",
    "  \n",
    "words = df_train['tokens'].apply(\n",
    "    lambda el_lst: pd.Series([el['text'] for el in el_lst])).stack().unique().tolist()\n",
    "words.append(\"ENDPAD\")\n",
    "\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "\n",
    "tags = df_train['crf_tags'].apply(lambda el_lst: pd.Series(el_lst)).stack().unique()\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = dict(map(reversed, tag2idx.items()))\n",
    "\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "\n",
    "# Train\n",
    "X_train = [[word2idx[w['text']] for w in s] for s in df_train['tokens']]\n",
    "X_train = pad_sequences(maxlen=max_len, sequences=X_train, padding=\"post\", value=word2idx['ENDPAD'])\n",
    "\n",
    "y_train = [[tag2idx[t] for t in s] for s in df_train['crf_tags']]\n",
    "y_train_pad = pad_sequences(maxlen=max_len, sequences=y_train, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train_pad]\n",
    "\n",
    "# Test\n",
    "X_test = [[word2idx.get(w['text'], 0) for w in s] for s in df_test['tokens']]\n",
    "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\", value=0)\n",
    "\n",
    "y_test = [[tag2idx[t] for t in s] for s in df_test['crf_tags']]\n",
    "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2149
    },
    "colab_type": "code",
    "id": "gOOzoR70_bZ8",
    "outputId": "3d69f25a-cda3-4347-d71e-140521bcda76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 165, 20)           134160    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3300)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 165)               544665    \n",
      "=================================================================\n",
      "Total params: 678,825\n",
      "Trainable params: 678,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "5675/5675 [==============================] - 5s 914us/step - loss: 0.1417 - acc: 0.3249\n",
      "Epoch 2/50\n",
      "5675/5675 [==============================] - 5s 908us/step - loss: 0.1152 - acc: 0.4374\n",
      "Epoch 3/50\n",
      "5675/5675 [==============================] - 5s 934us/step - loss: 0.0960 - acc: 0.5491\n",
      "Epoch 4/50\n",
      "5675/5675 [==============================] - 6s 1ms/step - loss: 0.0853 - acc: 0.5554\n",
      "Epoch 5/50\n",
      "5675/5675 [==============================] - 5s 857us/step - loss: 0.0800 - acc: 0.5655\n",
      "Epoch 6/50\n",
      "5675/5675 [==============================] - 5s 877us/step - loss: 0.0773 - acc: 0.5667\n",
      "Epoch 7/50\n",
      "5675/5675 [==============================] - 5s 871us/step - loss: 0.0757 - acc: 0.5676\n",
      "Epoch 8/50\n",
      "5675/5675 [==============================] - 5s 865us/step - loss: 0.0746 - acc: 0.5695\n",
      "Epoch 9/50\n",
      "5675/5675 [==============================] - 5s 867us/step - loss: 0.0740 - acc: 0.5669\n",
      "Epoch 10/50\n",
      "5675/5675 [==============================] - 5s 864us/step - loss: 0.0735 - acc: 0.5651\n",
      "Epoch 11/50\n",
      "5675/5675 [==============================] - 5s 868us/step - loss: 0.0731 - acc: 0.5639\n",
      "Epoch 12/50\n",
      "5675/5675 [==============================] - 5s 868us/step - loss: 0.0728 - acc: 0.5649\n",
      "Epoch 13/50\n",
      "5675/5675 [==============================] - 5s 820us/step - loss: 0.0726 - acc: 0.5602\n",
      "Epoch 14/50\n",
      "5675/5675 [==============================] - 5s 862us/step - loss: 0.0724 - acc: 0.5588\n",
      "Epoch 15/50\n",
      "5675/5675 [==============================] - 5s 864us/step - loss: 0.0722 - acc: 0.5559\n",
      "Epoch 16/50\n",
      "5675/5675 [==============================] - 5s 859us/step - loss: 0.0721 - acc: 0.5519\n",
      "Epoch 17/50\n",
      "5675/5675 [==============================] - 5s 857us/step - loss: 0.0720 - acc: 0.5530\n",
      "Epoch 18/50\n",
      "5675/5675 [==============================] - 5s 916us/step - loss: 0.0719 - acc: 0.5491\n",
      "Epoch 19/50\n",
      "5675/5675 [==============================] - 5s 879us/step - loss: 0.0718 - acc: 0.5507\n",
      "Epoch 20/50\n",
      "5675/5675 [==============================] - 5s 871us/step - loss: 0.0717 - acc: 0.5498\n",
      "Epoch 21/50\n",
      "5675/5675 [==============================] - 5s 868us/step - loss: 0.0717 - acc: 0.5424\n",
      "Epoch 22/50\n",
      "5675/5675 [==============================] - 5s 867us/step - loss: 0.0716 - acc: 0.5456\n",
      "Epoch 23/50\n",
      "5675/5675 [==============================] - 5s 862us/step - loss: 0.0716 - acc: 0.5461\n",
      "Epoch 24/50\n",
      "5675/5675 [==============================] - 5s 868us/step - loss: 0.0715 - acc: 0.5429\n",
      "Epoch 25/50\n",
      "5675/5675 [==============================] - 5s 864us/step - loss: 0.0715 - acc: 0.5369\n",
      "Epoch 26/50\n",
      "5675/5675 [==============================] - 5s 869us/step - loss: 0.0715 - acc: 0.5378\n",
      "Epoch 27/50\n",
      "5675/5675 [==============================] - 5s 867us/step - loss: 0.0714 - acc: 0.5385\n",
      "Epoch 28/50\n",
      "5675/5675 [==============================] - 5s 859us/step - loss: 0.0714 - acc: 0.5313\n",
      "Epoch 29/50\n",
      "5675/5675 [==============================] - 5s 870us/step - loss: 0.0714 - acc: 0.5327\n",
      "Epoch 30/50\n",
      "5675/5675 [==============================] - 5s 863us/step - loss: 0.0714 - acc: 0.5293\n",
      "Epoch 31/50\n",
      "5675/5675 [==============================] - 5s 872us/step - loss: 0.0713 - acc: 0.5311\n",
      "Epoch 32/50\n",
      "5675/5675 [==============================] - 5s 894us/step - loss: 0.0713 - acc: 0.5279\n",
      "Epoch 33/50\n",
      "5675/5675 [==============================] - 5s 875us/step - loss: 0.0713 - acc: 0.5260\n",
      "Epoch 34/50\n",
      "5675/5675 [==============================] - 5s 855us/step - loss: 0.0713 - acc: 0.5260\n",
      "Epoch 35/50\n",
      "5675/5675 [==============================] - 5s 865us/step - loss: 0.0713 - acc: 0.5202\n",
      "Epoch 36/50\n",
      "5675/5675 [==============================] - 5s 849us/step - loss: 0.0713 - acc: 0.5207\n",
      "Epoch 37/50\n",
      "5675/5675 [==============================] - 5s 865us/step - loss: 0.0713 - acc: 0.5212\n",
      "Epoch 38/50\n",
      "5675/5675 [==============================] - 5s 856us/step - loss: 0.0713 - acc: 0.5191\n",
      "Epoch 39/50\n",
      "5675/5675 [==============================] - 5s 856us/step - loss: 0.0713 - acc: 0.5218\n",
      "Epoch 40/50\n",
      "5675/5675 [==============================] - 5s 847us/step - loss: 0.0713 - acc: 0.5205\n",
      "Epoch 41/50\n",
      "5675/5675 [==============================] - 5s 850us/step - loss: 0.0712 - acc: 0.5216\n",
      "Epoch 42/50\n",
      "5675/5675 [==============================] - 5s 853us/step - loss: 0.0712 - acc: 0.5212\n",
      "Epoch 43/50\n",
      "5675/5675 [==============================] - 5s 855us/step - loss: 0.0712 - acc: 0.5233\n",
      "Epoch 44/50\n",
      "5675/5675 [==============================] - 5s 851us/step - loss: 0.0712 - acc: 0.5244\n",
      "Epoch 45/50\n",
      "5675/5675 [==============================] - 5s 843us/step - loss: 0.0712 - acc: 0.5228\n",
      "Epoch 46/50\n",
      "5675/5675 [==============================] - 5s 891us/step - loss: 0.0712 - acc: 0.5253\n",
      "Epoch 47/50\n",
      "5675/5675 [==============================] - 5s 904us/step - loss: 0.0712 - acc: 0.5219\n",
      "Epoch 48/50\n",
      "5675/5675 [==============================] - 5s 868us/step - loss: 0.0712 - acc: 0.5202\n",
      "Epoch 49/50\n",
      "5675/5675 [==============================] - 5s 848us/step - loss: 0.0712 - acc: 0.5209\n",
      "Epoch 50/50\n",
      "5675/5675 [==============================] - 5s 871us/step - loss: 0.0712 - acc: 0.5184\n"
     ]
    }
   ],
   "source": [
    "weights = embedding_weights(\n",
    "            X_train, y_train_pad,\n",
    "            n_words, n_tags,\n",
    "            max_len, emb_dim,\n",
    "            output_dir,\n",
    "            pretrained_emb_dir,\n",
    "            config_training['EMB_EPOCHS'],\n",
    "            'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FO8ttWOPKFQh",
    "outputId": "1792f6fe-77a4-4570-870d-a91497dadcaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6707"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(weights).shape\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuwNKeuDYehK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "  '''Callback to save model after each epoch and show training parameters '''\n",
    "  '''def __init__(self, savedir):\n",
    "      self.savedir = savedir\n",
    "      self.epoch = 0\n",
    "\n",
    "      os.makedirs(self.savedir, exist_ok=True)\n",
    "  '''\n",
    "  def on_epoch_end(self, model):\n",
    "      print(\"Model loss:\", model.get_latest_training_loss())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIlJQ_XOPhmg"
   },
   "outputs": [],
   "source": [
    "idx2word = dict(map(reversed, word2idx.items()))\n",
    "def reverseX(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            #p_i = np.argmax(p)\n",
    "            out_i.append(idx2word[p])\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "x2_train = reverseX(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "bGFS5qa3CcLV",
    "outputId": "2bd25d21-544b-466a-c581-bdb33748b88f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: 0.0\n",
      "Model loss: 0.0\n",
      "Model loss: 0.0\n",
      "Model loss: 0.0\n",
      "Model loss: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(698537, 4681875)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     #sample=6e-5, \n",
    "                     alpha=0.5,\n",
    "                     min_alpha=0.001, \n",
    "                     #min_alpha=0.0007, \n",
    "                     negative=0,\n",
    "                     compute_loss=True,\n",
    "                     sg=0)\n",
    "tokens = df_train['tokens'].apply(lambda x: [el['text'] for el in x])\n",
    "w2v_model.build_vocab(x2_train)\n",
    "w2v_model.train(x2_train, total_examples=w2v_model.corpus_count, epochs=5, start_alpha=0.5,end_alpha=0.5,compute_loss=True,callbacks=[EpochSaver()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-VJjStaAYOFx",
    "outputId": "d67efe41-9599-4b95-ec5a-25a19505d66d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Cwr5teI1Cuw9",
    "outputId": "9fa66932-6619-4c19-b756-748992cc46dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)\n",
    "w2v_model.save('/content/emb_model.bin')\n",
    "w = Word2Vec.load('/content/emb_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwHTFdxTU2Ld"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/drug_embeddings/resources/glove/wikipedia-pubmed-and-PMC-w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdSxeGt5U-OP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzUqoMUl9Ifx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_tst, y_tr, y_tst = train_test_split(\n",
    "            X_train , np.array(y_train), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "id": "gmrQmh7aRehG",
    "outputId": "39666cc1-98d6-4c56-a264-da025d2f819c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 165, 20)           134160    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3300)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 165)               544665    \n",
      "=================================================================\n",
      "Total params: 678,825\n",
      "Trainable params: 678,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "5675/5675 [==============================] - 2s 434us/step - loss: 0.1418 - acc: 0.3262\n",
      "Epoch 2/3\n",
      "5675/5675 [==============================] - 2s 399us/step - loss: 0.1148 - acc: 0.4465\n",
      "Epoch 3/3\n",
      "5675/5675 [==============================] - 2s 410us/step - loss: 0.0946 - acc: 0.5390\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "#emb_type = config['emb_type']\n",
    "#neurons_rnn = config['neurons_rnn']\n",
    "#neurons_dense = config['neurons_dense']\n",
    "#rec_drop = config['rec_drop']\n",
    "#impl = config['impl']\n",
    "#if emb_type == 'simple':\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_words+1,\n",
    "                    output_dim=emb_dim,\n",
    "                    input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(max_len, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "            \n",
    "model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "\n",
    "cbacks = []\n",
    "cbacks.append(metrics)\n",
    "tensorboard = TensorBoard(log_dir=\"{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "\n",
    "h = model.fit(X_train, y_train_pad,\n",
    "          #validation_data=(x_tst,y_tst),\n",
    "          epochs=3,\n",
    "          #callbacks=cbacks,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UILHHQ0XTO6e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Drug_EMB.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
